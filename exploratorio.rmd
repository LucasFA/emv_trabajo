---
title: "Práctica final"
author: "Pilar Ruiz Navarro, Lucas Fehlau Arbulu"
date: "`r Sys.Date()`"
output:
  html_document: 
    toc: true
    toc_depth: 2
    number_sections: false
    keep_md: false
    toc_float: 
      collapsed: false
      smooth_scroll: false
  pdf_document:
    keep_tex: false
---

<!--
Esquema del trabajo de acuerdo al guión:
# Análisis exploratorio univariante
## a) Recodificaciones/agrupaciones de datos/variables (preprocesado)
## b) Valores perdidos
## c) Análisis descriptivo (summary, código de clase)
## d) Outliers (código de clase)
## e) Comprobar normalidad. qqplot
## f) Otros

# Análisis exploratorio multivariante
## a)Correlación (código de clase. test de Bartlett)
## b) Si se han tratado los outliers, ACP (código de clase)
## c) Si no se han tratado los NA, (código de clase)
## d) Estudio posibilidad de aplicar ACP y aplicarlo(código de clase)
## e) Elegir número de variables y aplicar AF(código de clase)
## f) Normalidad multivariante (código de clase, tema 5)
## g) Clasificador (código de clase)
### Lineal
### Cuadrático
## h) Validar modelos 
## i) Análisis cluster
-->

<!-- 
Esquema del trabajo de ejemplo de Prado:
# Preprocesamiento de datos
## Descripción
## Carga de datos
## Valores perdidos
## Análisis exploratorio
## Outliers

# ACP
# AF

# AD
## Lineal
## Cuadrático
-->


# Abstract 
<!-- Opcional, supongo -->


# Introducción


# Preprocesado
## Lectura de los datos
```{r}
set.seed(314)
data <- read.csv("./data/marketing_customer.csv",
  sep = "\t",
  stringsAsFactors = TRUE
)
```

## Descripción de los datos
Esta es la descripción literal obtenida de Kaggle junto al conjunto de datos, presentado en inglés,
como originalmente

Personas:

* ID: Customer's unique identifier
* Year_Birth: Customer's birth year
* Education: Customer's education level
* Marital_Status: Customer's marital status
* Income: Customer's yearly household income
* Kidhome: Number of children in customer's household
* Teenhome: Number of teenagers in customer's household
* Dt_Customer: Date of customer's enrollment with the company
* Recency: Number of days since customer's last purchase
* Complain: 1 if the customer complained in the last 2 years, 0 otherwise

Productos:

* MntWines: Amount spent on wine in last 2 years
* MntFruits: Amount spent on fruits in last 2 years
* MntMeatProducts: Amount spent on meat in last 2 years
* MntFishProducts: Amount spent on fish in last 2 years
* MntSweetProducts: Amount spent on sweets in last 2 years
* MntGoldProds: Amount spent on gold in last 2 years

Promociones

* NumDealsPurchases: Number of purchases made with a discount
* AcceptedCmp1: 1 if customer accepted the offer in the 1st campaign, 0 otherwise
* AcceptedCmp2: 1 if customer accepted the offer in the 2nd campaign, 0 otherwise
* AcceptedCmp3: 1 if customer accepted the offer in the 3rd campaign, 0 otherwise
* AcceptedCmp4: 1 if customer accepted the offer in the 4th campaign, 0 otherwise
* AcceptedCmp5: 1 if customer accepted the offer in the 5th campaign, 0 otherwise
* Response: 1 if customer accepted the offer in the last campaign, 0 otherwise

Lugar de transacción (online, catálogo, tienda física)

* NumWebPurchases: Number of purchases made through the company’s website
* NumCatalogPurchases: Number of purchases made using a catalogue
* NumStorePurchases: Number of purchases made directly in stores
* NumWebVisitsMonth: Number of visits to company’s website in the last month

Objetivo

Aplicar clustering para segmentar a la clientela. 

<!-- Apartado c analisis descriptivo -->
## Analisis descriptivo
```{r warning=FALSE }
str(data)
colnames(data)
summary(data)
```

## Procesado

De acuerdo a lo anterior, ya vamos a eliminar ciertas columnas del conjunto de datos. En el caso de la fecha, por complicar el tratamiento de los datos. El ID, por ser irrelevante, mientras Z_CostContact y Z_Revenue son ambas constantes: en el summary sus valores máximos y mínimos son iguales.

```{r}
data <- data[, -which(names(data) %in% c("ID", "Dt_Customer", "Z_CostContact", "Z_Revenue"))]
```

<!-- Apartado b -->
## Valores perdidos
<!-- b.i) -->
Porcentaje de datos faltantes por columnas:
<!-- Es en cada columna? TODO: -->
```{r}
colMeans(is.na(data)) * 100
```

Una única variable (income) presenta datos faltantes, en aproximadamente el 1% de casos. Omitiremos todos estos casos.

```{r}
data <- na.omit(data)
```

Vamos a determinar la naturaleza de las últimas columnas de la tabla

```{r}
for (idx in 17:23) {
  cat("Frecuencia de:", colnames(data)[idx], ", columna ", idx)
  print(table(data[, idx]))
  cat("\n")
}
```

Observamos que las columnas AcceptedCmp* tienen únicamente valores 0 o 1, correspondiente a si aceptan la oferta o no. Las vamos a codificar sumándolas y eliminándolas

```{r}
data$sumaAcceptedCmp <-
  data[, "AcceptedCmp1"] +
  data[, "AcceptedCmp2"] +
  data[, "AcceptedCmp3"] +
  data[, "AcceptedCmp4"] +
  data[, "AcceptedCmp5"]


data[, "AcceptedCmp1"] <- NULL
data[, "AcceptedCmp2"] <- NULL
data[, "AcceptedCmp3"] <- NULL
data[, "AcceptedCmp4"] <- NULL
data[, "AcceptedCmp5"] <- NULL
```

También vamos a sumar el número de hijos, niños y adolescentes, en el hogar:

```{r}
data$sumaHijos <- data$Kidhome + data$Teenhome
data$Kidhome <- NULL
data$Teenhome <- NULL
```

También vamos a eliminar el año de nacimiento
```{r}
data$Year_Birth <- NULL
```

Con lo cual nos queda el siguiente conjunto de variables
```{r}
data_explicativas <- subset(data, select = -c(Complain, Response, sumaAcceptedCmp))
str(data_explicativas)
var_no_numericas <- data[, !sapply(data, is.numeric)]
var_numericas <- data[, sapply(data, is.numeric)]
var_categoricas <- data.frame(var_numericas)[, 14:17]
var_numericas_normalizado <- data.frame(scale(var_numericas, center = FALSE))[, 1:13]
print(length(var_numericas_normalizado))
```

<!-- Si más del 5%, analizar si es completely at random: -->

<!-- lapply(df, print_factor_frequency) -->
<!-- lapply(data, is.factor) -->
<!-- for (col in data) { -->
<!--   if (is.factor(data$col)) { -->
<!--     print("Frequency for", col, ":") -->
<!--     print(table(data[[col]])) -->
<!--     cat("\n") -->
<!--   } -->
<!-- } -->

<!-- apartado d  -->
## Outliers
```{r warning=FALSE, message=FALSE}
boxplot(
  var_numericas_normalizado,
  col = c(seq_len(ncol(var_numericas_normalizado))),
  main = "Outliers",
  xlab = "Variables explicativas",
  ylab = "Valor"
)
```

El resto de variables, por otro lado, son variables discretas, con lo que tienen muchos outliers.

```{r}
boxplot(
  var_categoricas,
  col = c(seq_len(ncol(var_categoricas))),
  main = "Outliers",
  xlab = "Variables explicativas",
  ylab = "Valor"
)

for (idx in seq_along(var_categoricas)) {
  cat("Frecuencia relativa de:", colnames(var_categoricas)[idx], ", columna ", idx)
  print(table(var_categoricas[, idx]) / length(var_categoricas[, idx]))
  cat("\n")
}
```

<!-- TODO: estudiar cada una -->
<!-- TODO: eliminar todos los outliers y ya -->

<!-- Apartado 1e) -->
## Normalidad univariante
<!-- qqplots para variables continuas, etc -->

Para comprobar la normalidad univariante he tomado varias transformaciones de los datos. 
```{r}
# rango <- seq(min(df[, idx]), max(df[, idx]), le = 50)
# for (i in 1:2) {
# i0 <- levels(datos$respuesta)[i]
# x <- datos[datos$respuesta == i0, nombre]
# }
pipeline <- function(transformation) {
  df <- var_numericas_normalizado
  for (idx in seq_along(df)) {
    nombre <- names(df)[idx]
    qqnorm(sqrt(var_numericas_normalizado[, idx]), main = paste("Variable", nombre), pch = 19, col = idx + 1)
    qqline(sqrt(var_numericas_normalizado[, idx]))
  }
}

par(mfrow = c(2, 2))
pipeline(identity)
# pipeline(log)
# pipeline(sqrt)
# TODO: descomentar
```

A primera vista no observamos ninguna mejora transformando las variables, con lo cual las dejamos en su estado original.
<!---->
<!-- ```{r} -->
<!-- for (colname in colnames(var_numericas_normalizado)) { -->
<!--   qqnorm( -->
<!--     y = var_numericas_normalizado[, colname], -->
<!--     main = c("Distribución de", colname), -->
<!--     col = "cornflowerblue", -->
<!--     cex.lab = 1.2, cex.main = 1.5, -->
<!--     xlab = "Cuantiles teóricos (distribución normal)", -->
<!--     ylab = "Cuantiles empíricos" -->
<!--   ) -->
<!--   qqline(var_numericas_normalizado[, colname], distribution = qnorm) -->
<!-- } -->
<!-- ``` -->
<!---->
Test de hipótesis
```{r}
for (colname in colnames(var_numericas_normalizado)) {
  print(shapiro.test(data[, colname]))
}
```

Las variables son claramente no normales.

<!-- apartado 2 -->
# Análisis exploratorio multivariante
<!-- apartado 2a -->
## Correlación
```{r}
library(psych)
cortest.bartlett(cor(var_numericas_normalizado), n = nrow(var_numericas_normalizado))
```

Naturalmente, teniendo que las distribuciones univariantes son no normales, la multivariante tampoco lo iba a ser.
<!-- apartado 2b -->
## Outliers 
<!-- TODO: todavía -->
(de nuevo. Se incluye o no dependiendo de como se haya tratado antes)

<!-- apartado 2c -->
<!-- ## Decisiones sobre NA (< 5 %) -->

<!-- apartado 2d -->
## ACP

### Viabilidad

### Obtención de las componentes principales
Usaremos la función `prcomp` del paquete `stats`.
```{r}
PCA <- prcomp(var_numericas_normalizado)
```

Dando como coeficientes:
```{r}
PCA$rotation
```
Visualizamos la fracción de la variance explicada por cada componente:
```{r}
plot(cumsum(PCA$sdev^2) / (sum(PCA$sdev^2)),
  type = "l",
  col = "blue",
  xlab = "Número de componentes", ylab = "Proporción de la varianza explicada",
  xlim = c(1, 7), ylim = c(0, 1)
)
lines(PCA$sdev^2 / (sum(PCA$sdev^2)), type = "l", col = "red")
```

La línea roja es la proporción de varianza explicada por cada componente, mientras la azul es la varianza explicada cumulativa.
Ya podemos ver que no vamos a necesitar más de 2 componentes

En formato de tabla:
```{r}
summary(PCA)
```

### Número de componentes principales
Usaremos la función `fviz_screeplot` del paquete `factoextra`.

```{r, results = FALSE, message=FALSE, warning=FALSE}
library(factoextra)
```

#### Método del codo
```{r}
fviz_screeplot(PCA, addlabels = TRUE)
```

#### Método de la media de la varianza
```{r}
PCA$sdev
mean(PCA$sdev^2)
```

Aunque una sola componente ya explica la mayoría de la varianza y la segunda componente principal sólo mejora la situación en un 10%, tomamos dos componentes.

### Visualización del ACP

```{r}
fviz_pca_var(PCA,
  axes = c(1, 2),
  repel = TRUE, col.var = "cos2",
  legend.title = "Distance"
) + theme_bw()
```

En esta gráfica apreciamos cómo las variables MntFruits, MntFishProducts, MntSweetProducts tienen una alta similaridad.

<!-- apartado e -->
## AF
### Preconditions
Visualizamos la matriz de correlaciones:
```{r}
poly_cor <- polycor::hetcor(var_numericas_normalizado)$correlations
ggcorrplot::ggcorrplot(poly_cor, type = "lower", hc.order = TRUE)
```

```{r}
corrplot::corrplot(cor(var_numericas_normalizado), order = "hclust", tl.col = "black", tl.cex = 1)
```

En estas gráficas también apreciamos la alta correlación entre las variables relacionadas con las compras. Por otro lado, el número de visitas a la web está negativamente relacionada con las compras. Esto no es una relación causal, naturalmente.

Las variables previamente mencionadas las vemos de nuevo altamente agrupadas, junto a, también, el número de compras por catálogo.

### Modelos

```{r}
modelo1 <- fa(poly_cor,
  nfactors = 2,
  rotate = "varimax",
  fm = "mle"
)
```

```{r}
modelo2 <- fa(poly_cor,
  nfactors = 4,
  rotate = "varimax",
  fm = "mle"
)
```

Obtenemos las comunalidades:
```{r }
c1 <- sort(modelo1$communality, decreasing = TRUE)
c2 <- sort(modelo2$communality, decreasing = TRUE)
cbind(c1, c2)
```

y las unicidades:
```{r }
u1 <- sort(modelo1$uniquenesses, decreasing = TRUE)
u2 <- sort(modelo2$uniquenesses, decreasing = TRUE)
cbind(u1, u2)
```

### Número de factores

Usaremos el scree plot y el análisis paralelo para asistirnos en la elección de factores.

```{r }
scree(poly_cor)
fa.parallel(
  poly_cor,
  n.obs = length(var_numericas_normalizado[, 1]),
  fa = "fa",
  fm = "mle"
)
```

Estos últimos métodos sugieren entre 2 y 4 factores


### Interpretación
```{r}
fa.diagram(modelo1)
fa.diagram(modelo2)
```

Atendiendo al segundo modelo, vemos que agrupa variables razonablemente conectadas.

<!-- Apartado f -->
<!-- necesario para A discriminante -->
## Normalidad multivariante

Primero comprobamos si hay outliers multivariantes:
TODO: data sería sin la variable explicada
```{r}
outliers <- MVN::mvn(data = var_numericas_normalizado[sample(nrow(var_numericas_normalizado), 1000), ], mvnTest = "hz", multivariateOutlierMethod = "quan")
```

TODO: comentar algo
```{r}
royston_test <- MVN::mvn(data = var_numericas_normalizado[sample(nrow(var_numericas_normalizado), 1000), ], mvnTest = "royston", multivariatePlot = "qq")

royston_test$multivariateNormality
```

TODO: data sería sin la variable explicada
```{r}
hz_test <- MVN::mvn(data = data, mvnTest = "hz")
hz_test$multivariateNormality
```


<!-- Apartado g -->

## Classifier

```{r}
# pairs(
#   x = data[sample(nrow(data), 100, replace = TRUE), ],
#   # TODO: creo que sería sin replace = true, sino false
#   col = c("green", "red")[data$class],
#   pch = 19
# )
```

#### Homogeneity of variance

In the Bartlett test we saw that the data is not spherically distributed, but it is also the case that the variances are not homogeneous as can be determined form the boxM test:

```{r}
library(biotools)
# boxM(
#   data = data[1:7],
#   grouping = data[, 8] # variable explicada
# )
```

