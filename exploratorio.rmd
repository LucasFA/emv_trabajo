---
title: "Práctica final"
author: "Pilar Ruiz Navarro, Lucas Fehlau Arbulu"
date: "`r Sys.Date()`"
output:
  html_document: 
    toc: true
    toc_depth: 2
    number_sections: false
    keep_md: false
    toc_float: 
      collapsed: false
      smooth_scroll: false
  pdf_document:
    keep_tex: false
---


Esquema del trabajo de acuerdo al guión:
# Análisis exploratorio univariante
## a) Recodificaciones/agrupaciones de datos/variables (preprocesado)
## b) Valores perdidos
## c) Análisis descriptivo (summary, código de clase)
## d) Outliers (código de clase)
## e) Comprobar normalidad. qqplot
## f) Otros

# Análisis exploratorio multivariante
## a)Correlación (código de clase. test de Bartlett)
## b) Si se han tratado los outliers, ACP (código de clase)
## c) Si no se han tratado los NA, (código de clase)
## d) Estudio posibilidad de aplicar ACP y aplicarlo(código de clase)
## e) Elegir número de variables y aplicar AF(código de clase)
## f) Normalidad multivariante (código de clase, tema 5)
## g) Clasificador (código de clase)
### Lineal
### Cuadrático
## h) Validar modelos 
## i) Análisis cluster


<!-- 
Esquema del trabajo de ejemplo de Prado:
# Preprocesamiento de datos
## Descripción
## Carga de datos
## Valores perdidos
## Análisis exploratorio
## Outliers

# ACP
# AF

# AD
## Lineal
## Cuadrático
-->


# Abstract 
<!-- Opcional, supongo -->


# Introducción


# Preprocesado
## Lectura de los datos
```{r}
# data <- read.csv()
data <- data.frame(list(c(1, 2, 3, 4), c(4, 3, 2, 1)))
```

## Descripción de los datos
Las variables del conjunto de datos son:
- Esto
- Aquello
- Cosa interesante

<!-- Apartado b -->
## Valores perdidos
<!-- b.i) -->
Porcentaje de datos faltantes por columnas:
<!-- Es en cada columna? TODO: -->
```{r}
colMeans(is.na(data)) * 100
```

<!-- Si más del 5%, analizar si es completely at random: -->
<!-- Para ello, estudiar la homogeneidad según grupos (NA y no -->
<!-- NA) con otras variables. Si son continuas, con un test de student, si son cualitativas o -->
<!-- discretas con test de independencia Chi-cuadrado, etc. -->
<!-- (Investigar funciones para el contraste de medias como t.test(), etc. del lenguaje R.) -->


<!-- Apartado c analisis descriptivo -->
## Analisis descriptivo
```{r warning=FALSE, message=FALSE}
colnames(data)

head(data)
summary(data)

# freq(data$nombre_columna)
# freq(data$nombre_columna)
# freq(data$nombre_columna)
# freq(data$nombre_columna)
```
<!-- apartado d  -->
## Outliers
```{r warning=FALSE, message=FALSE}
vars <- data # TODO: quitar las variables no explicativas (si las hay)
boxplot(
  vars,
  main = "Outliers",
  xlab = "Variables explicativas",
  ylab = "Valor",
  col = c(seq_len(ncol(vars))) # numero de columnas!
)
```

<!-- Apartado 1e) -->
## Normalidad univariante
<!-- qqplots para variables continuas, etc -->
```{r}
datos <- data
par(mfrow = c(2, 3))
for (k in 2:4) {
  j0 <- names(datos)[k]
  x0 <- seq(min(datos[, k]), max(datos[, k]), le = 50)
  for (i in 1:2) {
    i0 <- levels(datos$especie)[i]
    x <- datos[datos$especie == i0, j0]
    qqnorm(x, main = paste("especie", i0, j0), pch = 19, col = i + 1)
    qqline(x)
  }
}
# Ejemplo una variable:
qqnorm(no_out_standard$insu,
  main = "Insulin",
  col = "cornflowerblue",
  cex.lab = 1.2, cex.main = 1.5,
  xlab = "Theoretical quantiles (normal distribution)",
  ylab = "Empirical quantiles",
)
qqline(no_out_standard$insu, distribution = qnorm)
```

Test de hipótesis
```{r}
shapiro.test(data[[1]])
shapiro.test(data[[2]])
# etc
```

<!-- apartado 2 -->
# Análisis exploratorio multivariante
<!-- apartado 2a -->
## Correlación
```{r}
library(psych)
dataset_normalizado <- scale(dataset)
cortest.bartlett(cor(dataset_normalizado), n = nrow(dataset_normalizado))
```
<!-- apartado 2b -->
## Outliers 
(de nuevo. Se incluye o no dependiendo de como se haya tratado antes)

<!-- apartado 2c -->
## Decisiones sobre NA (< 5 %)

<!-- apartado 2d -->
## ACP
### Viabilidad

Blah blah por estas razones
### Obtención de las componentes principales
Usaremos la función `prcomp` del paquete `stats`.
```{r}
cosa_a_cambiar <- data
PCA <- prcomp(cosa_a_cambiar, scale = TRUE, center = TRUE)
```

Dando como coeficientes:
```{r}
PCA$rotation
```
Visualizamos la fracción de la variance explicada por cada componente:
```{r}
plot(cumsum(PCA$sdev^2) / (sum(PCA$sdev^2)), type = "l",
      col = "lightblue",
      xlab = "Number of components", ylab = "Proportion of variance explained",
      # xlim = c(1, 7), ylim = c(0, 1)
)
lines(PCA$sdev^2 / (sum(PCA$sdev^2)), type = "l", col = "red")
```
La línea roja es la proporción de varianza explicada por cada componente, mientras la azul es la varianza explicada cumulativa.
<!-- The red line is the proportion of variance explained by each component, while the blue line represents the cumulative proportion of variance explained by the first $i$ components. -->

En formato de tabla:
```{r}
summary(PCA)
```

### Número de componentes principales
Usaremos la función `fviz_screeplot` del paquete `factoextra`.

```{r, results = FALSE, message=FALSE, warning=FALSE}
library(factoextra)
```

#### Método del codo
```{r}
fviz_screeplot(PCA, addlabels = TRUE)
```

#### Método de la media de la varianza
```{r}
PCA$sdev
mean(PCA$sdev^2)
```

TODO: interpretar y elegir número de componentes.

### Visualización del ACP

```{r}
fviz_pca_var(PCA,
  repel = TRUE, col.var = "cos2",
  legend.title = "Distance"
) + theme_bw()
```

```{r}
fviz_pca_var(PCA,
  axes = c(1, 3),
  repel = TRUE, col.var = "cos2",
  legend.title = "Distance"
) + theme_bw()
```

<!-- apartado e -->
## AF
### Preconditions
Visualizamos la matriz de correlaciones:
```{r}
poly_cor <- polycor::hetcor(diab_no_output)$correlations
ggcorrplot::ggcorrplot(poly_cor, type = "lower", hc.order = TRUE)
```

```{r}
corrplot::corrplot(cor(diab_no_output), order = "hclust", tl.col = "black", tl.cex = 1)
```

Observamos algo? Algún número de factores?

Suponiendo 3 por ejemplo, tenemos:

### Modelos

Modelo por máxima verosimilitud:
```{r}
model1 <- fa(poly_cor,
  nfactors = 3,
  rotate = "none",
  fm = "mle"
)
```

Modelo por mínimos residuos:
```{r}
model2 <- fa(poly_cor,
  nfactors = 3,
  rotate = "none",
  fm = "minres"
)
```

Obtenemos las comunalidades:
```{r }
c1 <- sort(model1$communality, decreasing = TRUE)
c2 <- sort(model2$communality, decreasing = TRUE)
cbind(c1, c2)
```

y las unicidades:
```{r }
u1 <- sort(model1$uniquenesses, decreasing = TRUE)
u2 <- sort(model2$uniquenesses, decreasing = TRUE)
cbind(u1, u2)
```

### Número de factores

Usaremos el scree plot y el análisis paralelo para asistirnos en la elección de factores.

```{r }
scree(poly_cor)
fa.parallel(poly_cor, n.obs = length(diab_no_output[, 1]), fa = "fa", fm = "mle")
```

TODO: número de factores sugeridos?

### Interpretación
```{r}
fa.diagram(model1)
```

<!-- Apartado f -->
<!-- necesario para A discriminante -->
## Normalidad multivariante

Primero comprobamos si hay outliers multivariantes:
TODO: data sería sin la variable explicada
```{r}
outliers <- MVN::mvn(data = data, mvnTest = "hz", multivariateOutlierMethod = "quan")
```

TODO: comentar algo
```{r}
royston_test <- MVN::mvn(data = data, mvnTest = "royston", multivariatePlot = "qq")

royston_test$multivariateNormality
```

Likewise with the Henze-Zirkler test:
TODO: data sería sin la variable explicada
```{r}
hz_test <- MVN::mvn(data = data, mvnTest = "hz")
hz_test$multivariateNormality
```


<!-- Apartado g -->

## Classifier

```{r}
pairs(
  x = data[sample(nrow(data), 100), ],
  col = c("green", "red")[data$class],
  pch = 19
)
```

#### Homogeneity of variance

In the Bartlett test we saw that the data is not spherically distributed, but it is also the case that the variances are not homogeneous as can be determined form the boxM test:

```{r}
library(biotools)
boxM(
  data = data[1:7],
  grouping = data[, 8] # variable explicada
)
```

