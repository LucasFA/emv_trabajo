---
title: "Práctica final"
author: "Pilar Ruiz Navarro, Lucas Fehlau Arbulu"
date: "`r Sys.Date()`"
output:
  html_document: 
    toc: true
    toc_depth: 2
    number_sections: false
    keep_md: false
    toc_float: 
      collapsed: false
      smooth_scroll: false
  pdf_document:
    keep_tex: false
---

<!--
Esquema del trabajo de acuerdo al guión:
# Análisis exploratorio univariante
## a) Recodificaciones/agrupaciones de datos/variables (preprocesado)
## b) Valores perdidos
## c) Análisis descriptivo (summary, código de clase)
## d) Outliers (código de clase)
## e) Comprobar normalidad. qqplot
## f) Otros

# Análisis exploratorio multivariante
## a) Correlación (código de clase. test de Bartlett)
## b) Si se han tratado los outliers, ACP (código de clase)
## c) Si no se han tratado los NA, (código de clase)
## d) Estudio posibilidad de aplicar ACP y aplicarlo(código de clase)
## e) Elegir número de variables y aplicar AF(código de clase)
## f) Normalidad multivariante (código de clase, tema 5)
## g) Clasificador (código de clase)
### Lineal
### Cuadrático
## h) Validar modelos 
## i) Análisis cluster
-->

<!-- 
Esquema del trabajo de ejemplo de Prado:
# Preprocesamiento de datos
## Descripción
## Carga de datos
## Valores perdidos
## Análisis exploratorio
## Outliers

# ACP
# AF

# AD
## Lineal
## Cuadrático
-->


<!-- # Introducción, escrita directamente -->
Para este análisis hemos obtenido desde esta [página de Kaggle](https://www.kaggle.com/datasets/imakash3011/customer-personality-analysis/data) un dataset de análisis de la personalidad de clientes. Empezaremos con un análisis exploratorio, seguido de análisis de componentes principales y factorial. Finalmente acabamos creando un modelo de clasificación utilizando tanto un modelo lineal como cuadrático.


# Preprocesado
## Lectura de los datos
```{r}
set.seed(314)
data <- read.csv("./data/marketing_customer.csv",
  sep = "\t",
  stringsAsFactors = TRUE
)
```

## Descripción de los datos
Esta es la descripción literal obtenida de Kaggle junto al conjunto de datos, presentado en inglés,
como originalmente

Personas:

* ID: Customer's unique identifier
* Year_Birth: Customer's birth year
* Education: Customer's education level
* Marital_Status: Customer's marital status
* Income: Customer's yearly household income
* Kidhome: Number of children in customer's household
* Teenhome: Number of teenagers in customer's household
* Dt_Customer: Date of customer's enrollment with the company
* Recency: Number of days since customer's last purchase
* Complain: 1 if the customer complained in the last 2 years, 0 otherwise

Productos:

* MntWines: Amount spent on wine in last 2 years
* MntFruits: Amount spent on fruits in last 2 years
* MntMeatProducts: Amount spent on meat in last 2 years
* MntFishProducts: Amount spent on fish in last 2 years
* MntSweetProducts: Amount spent on sweets in last 2 years
* MntGoldProds: Amount spent on gold in last 2 years

Promociones

* NumDealsPurchases: Number of purchases made with a discount
* AcceptedCmp1: 1 if customer accepted the offer in the 1st campaign, 0 otherwise
* AcceptedCmp2: 1 if customer accepted the offer in the 2nd campaign, 0 otherwise
* AcceptedCmp3: 1 if customer accepted the offer in the 3rd campaign, 0 otherwise
* AcceptedCmp4: 1 if customer accepted the offer in the 4th campaign, 0 otherwise
* AcceptedCmp5: 1 if customer accepted the offer in the 5th campaign, 0 otherwise
* Response: 1 if customer accepted the offer in the last campaign, 0 otherwise

Lugar de transacción (online, catálogo, tienda física)

* NumWebPurchases: Number of purchases made through the company’s website
* NumCatalogPurchases: Number of purchases made using a catalogue
* NumStorePurchases: Number of purchases made directly in stores
* NumWebVisitsMonth: Number of visits to company’s website in the last month

Objetivo

Aplicar clustering para segmentar a la clientela. 

<!-- Apartado c analisis descriptivo -->
## Analisis descriptivo
```{r warning=FALSE }
str(data)
colnames(data)
summary(data)
```

## Procesado

De acuerdo a lo anterior, ya vamos a eliminar ciertas columnas del conjunto de datos. En el caso de la fecha, por complicar el tratamiento de los datos. El ID, por ser irrelevante, mientras Z_CostContact y Z_Revenue son ambas constantes: en el summary sus valores máximos y mínimos son iguales.

```{r}
data <- data[, -which(names(data) %in% c("ID", "Dt_Customer", "Z_CostContact", "Z_Revenue"))]
```

<!-- Apartado b -->
## Valores perdidos
<!-- b.i) -->
Porcentaje de datos faltantes por columnas:
<!-- Es en cada columna? TODO: -->
```{r}
colMeans(is.na(data)) * 100
```

Una única variable (income) presenta datos faltantes, en aproximadamente el 1% de casos. Omitiremos todos estos casos.

```{r}
data <- data[complete.cases(data), ]
```

Vamos a determinar la naturaleza de las últimas columnas de la tabla

```{r}
for (idx in 17:23) {
  cat("Frecuencia de:", colnames(data)[idx], ", columna ", idx)
  print(table(data[, idx]))
  cat("\n")
}
```

Observamos que las columnas AcceptedCmp* tienen únicamente valores 0 o 1, correspondiente a si aceptan la oferta o no. Las vamos a codificar sumándolas y eliminándolas

```{r}
data$sumaAcceptedCmp <-
  data[, "AcceptedCmp1"] +
  data[, "AcceptedCmp2"] +
  data[, "AcceptedCmp3"] +
  data[, "AcceptedCmp4"] +
  data[, "AcceptedCmp5"]


data[, "AcceptedCmp1"] <- NULL
data[, "AcceptedCmp2"] <- NULL
data[, "AcceptedCmp3"] <- NULL
data[, "AcceptedCmp4"] <- NULL
data[, "AcceptedCmp5"] <- NULL
```

También vamos a sumar el número de hijos, niños y adolescentes, en el hogar:

```{r}
data$sumaHijos <- data$Kidhome + data$Teenhome
data$Kidhome <- NULL
data$Teenhome <- NULL
```

También vamos a eliminar el año de nacimiento
```{r}
data$Year_Birth <- NULL
```

Con lo cual nos queda el siguiente conjunto de variables
```{r}
data_explicativas <- subset(data, select = -c(Complain, Response, sumaAcceptedCmp))
str(data_explicativas)

var_no_numericas <- data[, !sapply(data, is.numeric)]
var_numericas <- data[, sapply(data, is.numeric)]

var_categoricas <- data.frame(var_numericas)[, 14:17]
str(var_categoricas)

var_numericas_normalizado <- data.frame(scale(var_numericas))[, 1:13]
str(var_numericas_normalizado)
```

<!-- Si más del 5%, analizar si es completely at random: -->

<!-- lapply(df, print_factor_frequency) -->
<!-- lapply(data, is.factor) -->
<!-- for (col in data) { -->
<!--   if (is.factor(data$col)) { -->
<!--     print("Frequency for", col, ":") -->
<!--     print(table(data[[col]])) -->
<!--     cat("\n") -->
<!--   } -->
<!-- } -->

<!-- apartado d  -->
## Outliers
```{r warning=FALSE, message=FALSE}
boxplot(
  var_numericas_normalizado,
  col = c(seq_len(ncol(var_numericas_normalizado))),
  main = "Outliers",
  xlab = "Variables explicativas",
  ylab = "Valor"
)
```

El resto de variables, por otro lado, son variables discretas, con lo que tienen muchos outliers.

```{r}
boxplot(
  var_categoricas,
  col = c(seq_len(ncol(var_categoricas))),
  main = "Outliers",
  xlab = "Variables explicativas",
  ylab = "Valor"
)

for (idx in seq_along(var_categoricas)) {
  cat("Frecuencia relativa de:", colnames(var_categoricas)[idx], ", columna ", idx)
  print(table(var_categoricas[, idx]) / length(var_categoricas[, idx]))
  cat("\n")
}
```

Vamos a eliminar los outliers más extremos de las variables numéricas con la siguiente función:

```{r}
outlier <- function(data) {
  H <-1.5*IQR(data)
  data[data<quantile(data,0.25,na.rm = T)-H]<-NA
  data[data>quantile(data,0.75, na.rm = T)+H]<-NA
  data[is.na(data)]<-mean(data, na.rm = T)
  H<-1.5*IQR(data)

  if ( any(
           (data < quantile(data,0.25,na.rm = T)-H) |
           (data > quantile(data,0.75,na.rm = T)+H)))
    outlier(data)
  else
    return(data)
}

var_numericas_normalizado <- as.data.frame(lapply(var_numericas_normalizado, outlier))
```
<!-- TODO: estudiar cada una -->
<!-- TODO: eliminar todos los outliers y ya -->

<!-- Apartado 1e) -->
## Normalidad univariante
<!-- qqplots para variables continuas, etc -->

Para comprobar la normalidad univariante he tomado varias transformaciones de los datos. 

```{r}
pipeline <- function(transformation) {
  df <- var_numericas_normalizado
  for (colname in colnames(df)) {
    qqnorm(transformation(var_numericas_normalizado[, colname]), main = paste("Variable", colname), pch = 19, col = idx + 1)
    qqline(transformation(var_numericas_normalizado[, colname]))
  }
}

par(mfrow = c(2, 2))
pipeline(identity)
```

A primera vista no observamos ninguna mejora transformando las variables, con lo cual las dejamos en su estado original.

Test de hipótesis:
```{r}
for (colname in colnames(var_numericas_normalizado)) {
  print(shapiro.test(data[, colname]))
}
```

Las variables son claramente no normales.

<!-- apartado 2 -->
# Análisis exploratorio multivariante
<!-- apartado 2a -->
## Correlación
```{r}
library(psych)
cortest.bartlett(cor(var_numericas_normalizado), n = nrow(var_numericas_normalizado))
```

Efectivamente tenemos que las variables no son independientes 

<!-- apartado 2b -->
## Outliers 
<!-- TODO: todavía -->
(de nuevo. Se incluye o no dependiendo de como se haya tratado antes)

<!-- apartado 2c -->
<!-- ## Decisiones sobre NA (< 5 %) -->

<!-- apartado 2d -->
# ACP

## Viabilidad

## Obtención de las componentes principales
Usaremos la función `prcomp` del paquete `stats`.
```{r}
PCA <- prcomp(var_numericas_normalizado)
```

Dando como coeficientes:
```{r}
PCA$rotation
```
Visualizamos la fracción de la variance explicada por cada componente:
```{r}
plot(cumsum(PCA$sdev^2) / (sum(PCA$sdev^2)),
  type = "l",
  col = "blue",
  xlab = "Número de componentes", ylab = "Proporción de la varianza explicada",
  xlim = c(1, 7), ylim = c(0, 1)
)
lines(PCA$sdev^2 / (sum(PCA$sdev^2)), type = "l", col = "red")
```

La línea roja es la proporción de varianza explicada por cada componente, mientras la azul es la varianza explicada cumulativa.
Ya podemos ver que no vamos a necesitar más de 2 componentes

En formato de tabla:
```{r}
summary(PCA)
```

## Número de componentes principales
Usaremos la función `fviz_screeplot` del paquete `factoextra`.

```{r, results = FALSE, message=FALSE, warning=FALSE}
library(factoextra)
```

### Método del codo
```{r}
fviz_screeplot(PCA, addlabels = TRUE)
```

### Método de la media de la varianza
```{r}
PCA$sdev
mean(PCA$sdev^2)
```

Estos métodos sugieren entre 2 y 3 componentes principales. Tomaremos 2 en adelante.

### Visualización del ACP

```{r}
fviz_pca_var(PCA,
  axes = c(1, 2),
  repel = TRUE, col.var = "cos2",
  legend.title = "Distance"
) + theme_bw()
```

En esta gráfica apreciamos cómo las variables MntFruits, MntFishProducts, MntSweetProducts tienen una alta similaridad.

<!-- apartado e -->
# AF
## Condiciones
Visualizamos la matriz de correlaciones:
```{r}
poly_cor <- polycor::hetcor(var_numericas_normalizado)$correlations
ggcorrplot::ggcorrplot(poly_cor, type = "lower", hc.order = TRUE)
```

```{r}
corrplot::corrplot(cor(var_numericas_normalizado), order = "hclust", tl.col = "black", tl.cex = 1)
```

En estas gráficas también apreciamos la alta correlación entre las variables relacionadas con las compras. Por otro lado, el número de visitas a la web está negativamente relacionada con las compras. Esto no es una relación causal, naturalmente.

Las variables previamente mencionadas las vemos de nuevo altamente agrupadas, junto a, también, el número de compras por catálogo.

## Modelos

```{r}
modelar <- function(nfactors) {
  fa(poly_cor,
            nfactors = nfactors,
            rotate = "varimax",
            fm = "mle"
  )
}

modelo2 <- modelar(2)
modelo3 <- modelar(3)
modelo4 <- modelar(4)
```

Obtenemos las comunalidades:
```{r }
c1 <- sort(modelo2$communality, decreasing = TRUE)
c2 <- sort(modelo3$communality, decreasing = TRUE)
c3 <- sort(modelo4$communality, decreasing = TRUE)
cbind(c1, c2, c3)
```

y las unicidades:
```{r }
u1 <- sort(modelo2$uniquenesses, decreasing = TRUE)
u2 <- sort(modelo3$uniquenesses, decreasing = TRUE)
u3 <- sort(modelo4$uniquenesses, decreasing = TRUE)
cbind(u1, u2, u3)
```

## Número de factores

Usaremos el scree plot y el análisis paralelo para asistirnos en la elección de factores.

```{r }
scree(poly_cor)
fa.parallel(
  poly_cor,
  n.obs = length(var_numericas_normalizado[, 1]),
  fa = "fa",
  fm = "mle"
)
```

Estos últimos métodos sugieren entre 2 y 4 factores


## Interpretación
```{r}
fa.diagram(modelo2)
fa.diagram(modelo3)
fa.diagram(modelo4)
```

Atendiendo al segundo modelo, vemos que agrupa variables razonablemente conectadas.

<!-- Apartado f -->
<!-- necesario para A discriminante -->
## Normalidad multivariante

Primero comprobamos si hay outliers multivariantes:
```{r}
outliers <- MVN::mvn(data = var_numericas_normalizado[sample(nrow(var_numericas_normalizado), 1000), ], mvnTest = "hz", multivariateOutlierMethod = "quan")
```

```{r}
royston_test <- MVN::mvn(
  data = var_numericas_normalizado[sample(nrow(var_numericas_normalizado), 1000), ],
  mvnTest = "royston", multivariatePlot = "qq"
)
royston_test$multivariateNormality
```

```{r}
hz_test <- MVN::mvn(data = var_numericas_normalizado, mvnTest = "hz")
hz_test$multivariateNormality
```

Sabiendo que la distribución no es en absoluto normal, procedemos a intentar crear un clasificador en el conjunto de datos a pesar de ello.

<!-- Apartado g -->

# Clasificación

<!-- ```{r} -->
<!-- pairs( -->
<!--   x = var_numericas_normalizado[, c("")], -->
<!--   col = c("blue", "orange")[datos$especie], pch = 19 -->
<!-- ) -->
<!-- ``` -->

<!-- pairs( -->
<!--   x = data[sample(nrow(data), 100, replace = TRUE), ], -->
<!--   # TODO: creo que sería sin replace = true, sino false -->
<!--   col = c("green", "red")[data$class], -->
<!--   pch = 19 -->
<!-- ) -->
<!---->
## Homogeneidad de la varianza

Veamos si las varianzas son homogéneas, usando el test boxM. Podemos clasificar de acuerdo con varias variables. Una posibilidad es considerar si el cliente aceptó la última oferta.

```{r warning=FALSE, message=FALSE}
library(biotools)

boxM(
  data = var_numericas_normalizado,
  grouping = data$Response
)
```

Asumimos la homogeneidad de las varianzas, al haber rechazado el test de contraste.

## Modelos

Antes de modelar, hacemos un split entrenamiento/test 80/20:
```{r}
train_proporc <- 0.8
total <- nrow(var_numericas_normalizado)

train_size <- train_proporc * total
test_size <- total - train_size

final_data <- var_numericas_normalizado
final_data$Response <- data$Response

indices_entrenamiento <- sample(nrow(final_data), size = train_size)
datos_train <- final_data[indices_entrenamiento, ]
datos_test <- final_data[-indices_entrenamiento, ]
```

Para los modelos utilizaremos la siguiente fórmula:
```{r}
formula <- as.formula(paste("Response", "~", paste(colnames(var_numericas_normalizado), collapse = "+")))
print(formula)
```

Consideramos el modelo discriminante cuadrático siguiente:
```{r warning=FALSE, message=FALSE}
modelo_qda <- qda(formula, data = datos_train)
modelo_qda
```

También creamos un modelo discriminante lineal, pero ya sabemos por adelantado que las hipótesis necesarias para este modelo,
en particular la de normalidad multivariante, no se da.

```{r}
modelo_lda <- lda(formula, data = datos_train)
modelo_lda
```

## Validación

```{r}
prob_1 <- 1 == final_data$Response
pipeline_validacion <- function(modelo, testing_dataset) {
  prediccion <- predict(modelo, testing_dataset)

  # print(str(prediccion))
  error_medio <- mean(testing_dataset$Response != prediccion$class) * 100
  print(paste("Tasa de error media: ", error_medio, "%"))
}
```

```{r}
pipeline_validacion(modelo_qda, datos_test)
pipeline_validacion(modelo_lda, datos_test)
```

Se nos presenta un caso en el que a pesar de la mayor robustez del modelo cuadrático, el modelo lineal tiene una tasa de error razonable. 
Como nota de desarrollo sobre este trabajo, lo que hemos realizado es validación cruzada con $k = 1$. Típicamente se tomaría $k$ un valor más 
alto, por el cual se entrenarían los modelos con distintos conjuntos de entrenamiento, para ser cada uno evaluado y tomar así como
valor real del error la media de ellos, aportando mucha más robustez a esta figura.

Al mismo tiempo, lo comparamos con el clasificador más simple: el que devuelve la clase mayoritaria del conjunto de entrenamiento.

```{r}
table(datos_train$Response) / nrow(datos_train)
table(datos_test$Response) / nrow(datos_test)
```
Con lo cual la tasa de error media en el conjunto de testeo sería 14.4%. Este rendimiento es esencialmente el mismo que el obtenido en los modelos tanto cuadrático
como lineal, con lo cual su valor puramente predictivo es esencialmente nulo.

